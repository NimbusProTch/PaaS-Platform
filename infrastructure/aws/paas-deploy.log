[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_partition.current: Reading...[0m[0m
[0m[1mdata.aws_availability_zones.available: Reading...[0m[0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.vpc.data.aws_caller_identity.current[0]: Reading...[0m[0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_region.current: Reading...[0m[0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_partition.current: Reading...[0m[0m
[0m[1mdata.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.vpc.data.aws_region.current[0]: Reading...[0m[0m
[0m[1mmodule.vpc.data.aws_iam_policy_document.flow_log_cloudwatch_assume_role[0]: Reading...[0m[0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.vpc.data.aws_region.current[0]: Read complete after 0s [id=eu-west-1][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_region.current: Read complete after 0s [id=eu-west-1][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.vpc.data.aws_iam_policy_document.flow_log_cloudwatch_assume_role[0]: Read complete after 0s [id=1021377347][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.vpc.aws_vpc.this[0]: Refreshing state... [id=vpc-02aecc65be4eab79c][0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_region.current: Reading...[0m[0m
[0m[1maws_kms_key.eks: Refreshing state... [id=0918b70f-90ec-4bae-a1ad-8b2df8144049][0m
[0m[1mmodule.vpc.data.aws_partition.current[0]: Reading...[0m[0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_region.current: Read complete after 0s [id=eu-west-1][0m
[0m[1mmodule.vpc.data.aws_partition.current[0]: Read complete after 0s [id=aws][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_region.current: Reading...[0m[0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_region.current: Read complete after 0s [id=eu-west-1][0m
[0m[1mmodule.vpc.aws_iam_role.vpc_flow_log_cloudwatch[0]: Refreshing state... [id=vpc-flow-log-role-20251217164416803300000001][0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_iam_policy_document.ebs_csi[0]: Reading...[0m[0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_iam_policy_document.ebs_csi[0]: Read complete after 0s [id=4189668531][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_iam_policy_document.load_balancer_controller[0]: Reading...[0m[0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_iam_policy_document.load_balancer_controller[0]: Read complete after 0s [id=1541424006][0m
[0m[1mmodule.ebs_csi_driver_irsa[0].aws_iam_policy.ebs_csi[0]: Refreshing state... [id=arn:aws:iam::715841344657:policy/AmazonEKS_EBS_CSI_Policy-20251217170752617800000002][0m
[0m[1mdata.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].aws_iam_policy.load_balancer_controller[0]: Refreshing state... [id=arn:aws:iam::715841344657:policy/AmazonEKS_AWS_Load_Balancer_Controller-20251217170752620300000004][0m
[0m[1mmodule.vpc.data.aws_caller_identity.current[0]: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mdata.aws_availability_zones.available: Read complete after 0s [id=eu-west-1][0m
[0m[1maws_kms_alias.eks: Refreshing state... [id=alias/infraforge-dev-eks][0m
[0m[1mmodule.vpc.aws_default_security_group.this[0]: Refreshing state... [id=sg-0a71adffcb737b7be][0m
[0m[1mmodule.vpc.aws_default_route_table.default[0]: Refreshing state... [id=rtb-04c8a079d9f0c814e][0m
[0m[1mmodule.vpc.aws_subnet.public[2]: Refreshing state... [id=subnet-07335efb572ce6797][0m
[0m[1mmodule.vpc.aws_subnet.public[0]: Refreshing state... [id=subnet-085a97102a8d53877][0m
[0m[1maws_security_group.vpc_endpoints: Refreshing state... [id=sg-0ac17b07ac9ce0535][0m
[0m[1mmodule.vpc.aws_subnet.public[1]: Refreshing state... [id=subnet-0270e2a0b25214ee7][0m
[0m[1mmodule.vpc.aws_subnet.private[2]: Refreshing state... [id=subnet-0457fc5bd24280050][0m
[0m[1mmodule.vpc.aws_route_table.private[0]: Refreshing state... [id=rtb-0b6e76b6d97ba3be6][0m
[0m[1maws_vpc_endpoint.s3: Refreshing state... [id=vpce-0509f466e8cd91500][0m
[0m[1mmodule.vpc.aws_default_network_acl.this[0]: Refreshing state... [id=acl-0417b5d89acaea64b][0m
[0m[1mmodule.vpc.aws_internet_gateway.this[0]: Refreshing state... [id=igw-0af875bccf123c7f4][0m
[0m[1mmodule.vpc.aws_subnet.private[0]: Refreshing state... [id=subnet-0f97716901fab29c8][0m
[0m[1mmodule.vpc.aws_route_table.public[0]: Refreshing state... [id=rtb-026f55e0e62d90393][0m
[0m[1mmodule.vpc.aws_subnet.private[1]: Refreshing state... [id=subnet-0d2ae9b9e36b78d6a][0m
[0m[1mmodule.vpc.aws_cloudwatch_log_group.flow_log[0]: Refreshing state... [id=/aws/vpc-flow-log/vpc-02aecc65be4eab79c][0m
[0m[1mmodule.vpc.aws_subnet.database[0]: Refreshing state... [id=subnet-07f3edf73f72d3ba6][0m
[0m[1mmodule.vpc.aws_subnet.database[1]: Refreshing state... [id=subnet-0963fe39308643c5b][0m
[0m[1mmodule.vpc.aws_subnet.database[2]: Refreshing state... [id=subnet-0076980beb724a33c][0m
[0m[1mmodule.vpc.aws_eip.nat[0]: Refreshing state... [id=eipalloc-0882a1798fb2f63a5][0m
[0m[1mmodule.vpc.aws_route.public_internet_gateway[0]: Refreshing state... [id=r-rtb-026f55e0e62d903931080289494][0m
[0m[1mmodule.vpc.aws_route_table_association.public[2]: Refreshing state... [id=rtbassoc-0e4c04b4e1394c367][0m
[0m[1mmodule.vpc.aws_route_table_association.public[0]: Refreshing state... [id=rtbassoc-0e360f73a525cb9a4][0m
[0m[1mmodule.vpc.aws_route_table_association.public[1]: Refreshing state... [id=rtbassoc-09907922629a98161][0m
[0m[1mmodule.vpc.aws_route_table_association.private[1]: Refreshing state... [id=rtbassoc-00b250cc91ce00637][0m
[0m[1mmodule.vpc.aws_route_table_association.private[0]: Refreshing state... [id=rtbassoc-051e8465a059c4ad7][0m
[0m[1mmodule.vpc.aws_route_table_association.private[2]: Refreshing state... [id=rtbassoc-0468c37be337aec93][0m
[0m[1maws_vpc_endpoint.ecr_api: Refreshing state... [id=vpce-0a3f86458a9c396e5][0m
[0m[1maws_vpc_endpoint.ecr_dkr: Refreshing state... [id=vpce-0cdab1ac89e064304][0m
[0m[1maws_vpc_endpoint.ec2: Refreshing state... [id=vpce-04c405fd9a31c6570][0m
[0m[1maws_vpc_endpoint.sts: Refreshing state... [id=vpce-0c14afe0059c559bf][0m
[0m[1mmodule.vpc.aws_db_subnet_group.database[0]: Refreshing state... [id=infraforge-dev-vpc][0m
[0m[1mmodule.vpc.aws_route_table_association.database[2]: Refreshing state... [id=rtbassoc-0e52afb5c1a0790b3][0m
[0m[1mmodule.vpc.aws_route_table_association.database[0]: Refreshing state... [id=rtbassoc-0ff62d183965c3e04][0m
[0m[1mmodule.vpc.aws_route_table_association.database[1]: Refreshing state... [id=rtbassoc-0a1c29b9ffccd5334][0m
[0m[1mmodule.vpc.aws_nat_gateway.this[0]: Refreshing state... [id=nat-0170a690ae1827861][0m
[0m[1mmodule.vpc.data.aws_iam_policy_document.vpc_flow_log_cloudwatch[0]: Reading...[0m[0m
[0m[1mmodule.vpc.aws_flow_log.this[0]: Refreshing state... [id=fl-04b613010c8866abe][0m
[0m[1mmodule.vpc.data.aws_iam_policy_document.vpc_flow_log_cloudwatch[0]: Read complete after 0s [id=2841270105][0m
[0m[1mmodule.vpc.aws_iam_policy.vpc_flow_log_cloudwatch[0]: Refreshing state... [id=arn:aws:iam::715841344657:policy/vpc-flow-log-to-cloudwatch-20251217164431366900000005][0m
[0m[1mmodule.vpc.aws_route.private_nat_gateway[0]: Refreshing state... [id=r-rtb-0b6e76b6d97ba3be61080289494][0m
[0m[1mmodule.vpc.aws_iam_role_policy_attachment.vpc_flow_log_cloudwatch[0]: Refreshing state... [id=vpc-flow-log-role-20251217164416803300000001-20251217164432289400000006][0m
[0m[1mmodule.eks.data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.eks.time_sleep.this[0] (deposed object fca075fe): Refreshing state... [id=2025-12-17T17:18:25Z][0m
[0m[1mmodule.eks.data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.eks.module.fargate_profile["system"].data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.eks.module.fargate_profile["system"].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.eks.module.kms.data.aws_caller_identity.current[0]: Reading...[0m[0m
[0m[1mmodule.eks.aws_cloudwatch_log_group.this[0]: Refreshing state... [id=/aws/eks/infraforge-dev/cluster][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_iam_policy_document.assume_role_policy[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.fargate_profile["system"].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.eks.module.kms.data.aws_partition.current[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.fargate_profile["system"].data.aws_iam_policy_document.assume_role_policy[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.eks.module.kms.data.aws_partition.current[0]: Read complete after 0s [id=aws][0m
[0m[1mmodule.eks.data.aws_iam_policy_document.assume_role_policy[0]: Read complete after 0s [id=2764486067][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.eks.module.fargate_profile["system"].data.aws_iam_policy_document.assume_role_policy[0]: Read complete after 0s [id=3016102342][0m
[0m[1mmodule.eks.aws_iam_role.this[0]: Refreshing state... [id=infraforge-dev-cluster-20251217170752618600000003][0m
[0m[1mmodule.eks.aws_security_group.cluster[0]: Refreshing state... [id=sg-04590f98e62457e9a][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.eks.module.fargate_profile["system"].aws_iam_role.this[0]: Refreshing state... [id=system-20251217170752623600000006][0m
[0m[1mmodule.eks.aws_security_group.node[0]: Refreshing state... [id=sg-03b17afed4f4e12ed][0m
[0m[1mmodule.eks.data.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].module.user_data.data.cloudinit_config.linux_eks_managed_node_group[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].module.user_data.data.cloudinit_config.linux_eks_managed_node_group[0]: Read complete after 0s [id=302967155][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].data.aws_iam_policy_document.assume_role_policy[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].data.aws_iam_policy_document.assume_role_policy[0]: Read complete after 0s [id=2560088296][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].data.aws_iam_policy_document.assume_role_policy[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].data.aws_iam_policy_document.assume_role_policy[0]: Read complete after 0s [id=2560088296][0m
[0m[1mmodule.eks.data.aws_iam_session_context.current: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_iam_session_context.current: Read complete after 0s [id=arn:aws:iam::715841344657:user/gokhan][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].aws_iam_role.this[0]: Refreshing state... [id=infraforge-dev-system-eks-node-group-20251217170752624100000008][0m
[0m[1mmodule.eks.module.kms.data.aws_caller_identity.current[0]: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].aws_iam_role.this[0]: Refreshing state... [id=infraforge-dev-general-eks-node-group-20251217170752624000000007][0m
[0m[1mmodule.eks.module.fargate_profile["system"].data.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].data.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].data.aws_caller_identity.current: Read complete after 0s [id=715841344657][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_nodes_ephemeral"]: Refreshing state... [id=sgrule-1484434223][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_443"]: Refreshing state... [id=sgrule-2383273807][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_8443_webhook"]: Refreshing state... [id=sgrule-1557396504][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_self_coredns_tcp"]: Refreshing state... [id=sgrule-2905028476][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_4443_webhook"]: Refreshing state... [id=sgrule-2261624751][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_self_coredns_udp"]: Refreshing state... [id=sgrule-158897299][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_kubelet"]: Refreshing state... [id=sgrule-1024160259][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_9443_webhook"]: Refreshing state... [id=sgrule-947048403][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_6443_webhook"]: Refreshing state... [id=sgrule-1334820921][0m
[0m[1mmodule.eks.aws_security_group_rule.node["egress_all"]: Refreshing state... [id=sgrule-1296582488][0m
[0m[1mmodule.eks.aws_security_group_rule.cluster["ingress_nodes_443"]: Refreshing state... [id=sgrule-2698008309][0m
[0m[1mmodule.eks.module.fargate_profile["system"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"]: Refreshing state... [id=system-20251217170752623600000006-20251217170754119300000009][0m
[0m[1mmodule.eks.module.fargate_profile["system"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy"]: Refreshing state... [id=system-20251217170752623600000006-2025121717075414050000000a][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"]: Refreshing state... [id=infraforge-dev-system-eks-node-group-20251217170752624100000008-2025121717075417210000000c][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"]: Refreshing state... [id=infraforge-dev-system-eks-node-group-20251217170752624100000008-2025121717075421260000000d][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"]: Refreshing state... [id=infraforge-dev-system-eks-node-group-20251217170752624100000008-2025121717075416890000000b][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"]: Refreshing state... [id=infraforge-dev-general-eks-node-group-20251217170752624000000007-2025121717075432760000000f][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"]: Refreshing state... [id=infraforge-dev-general-eks-node-group-20251217170752624000000007-20251217170754585000000010][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"]: Refreshing state... [id=infraforge-dev-general-eks-node-group-20251217170752624000000007-2025121717075431440000000e][0m
[0m[1mmodule.eks.aws_iam_role_policy_attachment.this["AmazonEKSVPCResourceController"]: Refreshing state... [id=infraforge-dev-cluster-20251217170752618600000003-20251217170754671500000011][0m
[0m[1mmodule.eks.aws_iam_role_policy_attachment.this["AmazonEKSClusterPolicy"]: Refreshing state... [id=infraforge-dev-cluster-20251217170752618600000003-20251217170754695600000012][0m
[0m[1mmodule.eks.module.kms.data.aws_iam_policy_document.this[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.kms.data.aws_iam_policy_document.this[0]: Read complete after 0s [id=2339894789][0m
[0m[1mmodule.eks.module.kms.aws_kms_key.this[0]: Refreshing state... [id=cefb2c2d-7827-48b1-9381-d6581ea7b1f2][0m
[0m[1mmodule.eks.module.kms.aws_kms_alias.this["cluster"]: Refreshing state... [id=alias/eks/infraforge-dev][0m
[0m[1mmodule.eks.aws_iam_policy.cluster_encryption[0]: Refreshing state... [id=arn:aws:iam::715841344657:policy/infraforge-dev-cluster-ClusterEncryption20251217170817072900000013][0m
[0m[1mmodule.eks.aws_eks_cluster.this[0]: Refreshing state... [id=infraforge-dev][0m
[0m[1mmodule.eks.aws_iam_role_policy_attachment.cluster_encryption[0]: Refreshing state... [id=infraforge-dev-cluster-20251217170752618600000003-20251217170817969500000014][0m
[0m[1mmodule.eks.data.tls_certificate.this[0]: Reading...[0m[0m
[0m[1mmodule.eks.data.tls_certificate.this[0]: Read complete after 1s [id=08332733484502a5c0ee4f44b59e6f9baaa72352][0m
[0m[1mmodule.eks.aws_ec2_tag.cluster_primary_security_group["Environment"]: Refreshing state... [id=sg-0e62a90d72270fe34,Environment][0m
[0m[1mmodule.eks.aws_ec2_tag.cluster_primary_security_group["Tenant"]: Refreshing state... [id=sg-0e62a90d72270fe34,Tenant][0m
[0m[1mmodule.eks.aws_ec2_tag.cluster_primary_security_group["Cluster"]: Refreshing state... [id=sg-0e62a90d72270fe34,Cluster][0m
[0m[1mmodule.eks.time_sleep.this[0]: Refreshing state... [id=2025-12-17T19:14:21Z][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_iam_policy_document.cluster_autoscaler[0]: Reading...[0m[0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_iam_policy_document.cluster_autoscaler[0]: Read complete after 0s [id=3691530538][0m
[0m[1mmodule.eks.aws_iam_openid_connect_provider.oidc_provider[0]: Refreshing state... [id=arn:aws:iam::715841344657:oidc-provider/oidc.eks.eu-west-1.amazonaws.com/id/ADDA703F782CABA7FEBFF9FC355069D1][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].aws_iam_policy.cluster_autoscaler[0]: Refreshing state... [id=arn:aws:iam::715841344657:policy/AmazonEKS_Cluster_Autoscaler_Policy-20251217171755879000000015][0m
[0m[1mmodule.eks.module.fargate_profile["system"].aws_eks_fargate_profile.this[0]: Refreshing state... [id=infraforge-dev:system][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].aws_launch_template.this[0]: Refreshing state... [id=lt-0116fdd038f39ecc4][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].aws_launch_template.this[0]: Refreshing state... [id=lt-00d30dbbdb69d2c24][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_iam_policy_document.this[0]: Reading...[0m[0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_iam_policy_document.this[0]: Reading...[0m[0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_iam_policy_document.this[0]: Reading...[0m[0m
[0m[1mmodule.ebs_csi_driver_irsa[0].data.aws_iam_policy_document.this[0]: Read complete after 0s [id=3856997166][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].data.aws_iam_policy_document.this[0]: Read complete after 0s [id=3227648327][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].data.aws_iam_policy_document.this[0]: Read complete after 0s [id=1441892670][0m
[0m[1mmodule.ebs_csi_driver_irsa[0].aws_iam_role.this[0]: Refreshing state... [id=infraforge-dev-ebs-csi-driver][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].aws_iam_role.this[0]: Refreshing state... [id=infraforge-dev-cluster-autoscaler][0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].aws_iam_role.this[0]: Refreshing state... [id=infraforge-dev-aws-load-balancer-controller][0m
[0m[1mmodule.eks.module.eks_managed_node_group["system"].aws_eks_node_group.this[0]: Refreshing state... [id=infraforge-dev:infraforge-dev-system-20251217191421840400000003][0m
[0m[1mmodule.eks.module.eks_managed_node_group["general"].aws_eks_node_group.this[0]: Refreshing state... [id=infraforge-dev:infraforge-dev-general-20251217191421839500000001][0m
[0m[1mmodule.ebs_csi_driver_irsa[0].aws_iam_role_policy_attachment.ebs_csi[0]: Refreshing state... [id=infraforge-dev-ebs-csi-driver-20251217171759955400000017][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["coredns"]: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["kube-proxy"]: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["aws-ebs-csi-driver"]: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["vpc-cni"]: Reading...[0m[0m
[0m[1mmodule.aws_load_balancer_controller_irsa[0].aws_iam_role_policy_attachment.load_balancer_controller[0]: Refreshing state... [id=infraforge-dev-aws-load-balancer-controller-20251217171800082300000018][0m
[0m[1mmodule.cluster_autoscaler_irsa[0].aws_iam_role_policy_attachment.cluster_autoscaler[0]: Refreshing state... [id=infraforge-dev-cluster-autoscaler-20251217171759942000000016][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["kube-proxy"]: Read complete after 1s [id=kube-proxy][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["coredns"]: Read complete after 1s [id=coredns][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["vpc-cni"]: Read complete after 1s [id=vpc-cni][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["aws-ebs-csi-driver"]: Read complete after 1s [id=aws-ebs-csi-driver][0m
[0m[1mmodule.eks.aws_eks_addon.this["vpc-cni"]: Refreshing state... [id=infraforge-dev:vpc-cni][0m
[0m[1mmodule.eks.aws_eks_addon.this["kube-proxy"]: Refreshing state... [id=infraforge-dev:kube-proxy][0m
[0m[1mmodule.eks.aws_eks_addon.this["coredns"]: Refreshing state... [id=infraforge-dev:coredns][0m
[0m[1mmodule.eks.aws_eks_addon.this["aws-ebs-csi-driver"]: Refreshing state... [id=infraforge-dev:aws-ebs-csi-driver][0m

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  [32m+[0m create[0m
  [31m-[0m destroy[0m
 [36m<=[0m read (data resources)[0m

Terraform will perform the following actions:

[1m  # data.kubernetes_secret.backstage_sa_token[0m will be read during apply
  # (config refers to values not yet known)
[0m [36m<=[0m[0m data "kubernetes_secret" "backstage_sa_token" {
      [32m+[0m[0m data      = (sensitive value)
      [32m+[0m[0m id        = (known after apply)
      [32m+[0m[0m immutable = (known after apply)
      [32m+[0m[0m type      = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m name             = (known after apply)
          [32m+[0m[0m namespace        = "backstage"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }
    }

[1m  # helm_release.argocd[0m will be created
[0m  [32m+[0m[0m resource "helm_release" "argocd" {
      [32m+[0m[0m atomic                     = false
      [32m+[0m[0m chart                      = "argo-cd"
      [32m+[0m[0m cleanup_on_fail            = false
      [32m+[0m[0m create_namespace           = true
      [32m+[0m[0m dependency_update          = false
      [32m+[0m[0m disable_crd_hooks          = false
      [32m+[0m[0m disable_openapi_validation = false
      [32m+[0m[0m disable_webhooks           = false
      [32m+[0m[0m force_update               = false
      [32m+[0m[0m id                         = (known after apply)
      [32m+[0m[0m lint                       = false
      [32m+[0m[0m manifest                   = (known after apply)
      [32m+[0m[0m max_history                = 0
      [32m+[0m[0m metadata                   = (known after apply)
      [32m+[0m[0m name                       = "argocd"
      [32m+[0m[0m namespace                  = "argocd"
      [32m+[0m[0m pass_credentials           = false
      [32m+[0m[0m recreate_pods              = false
      [32m+[0m[0m render_subchart_notes      = true
      [32m+[0m[0m replace                    = false
      [32m+[0m[0m repository                 = "https://argoproj.github.io/argo-helm"
      [32m+[0m[0m reset_values               = false
      [32m+[0m[0m reuse_values               = false
      [32m+[0m[0m skip_crds                  = false
      [32m+[0m[0m status                     = "deployed"
      [32m+[0m[0m timeout                    = 300
      [32m+[0m[0m values                     = [
          [32m+[0m[0m <<-EOT
                global:
                  image:
                    tag: "v2.9.3"
                
                server:
                  service:
                    type: LoadBalancer
                    annotations:
                      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
                
                  config:
                    url: "https://argocd.infraforge.io"
                
                    # OIDC Configuration (optional)
                    oidc.config: |
                      name: AWS SSO
                      issuer: https://oidc.eks.eu-west-1.amazonaws.com/id/https://oidc.eks.eu-west-1.amazonaws.com/id/ADDA703F782CABA7FEBFF9FC355069D1
                      clientId: argocd
                      requestedScopes: ["openid", "profile", "email"]
                      requestedIdTokenClaims: {"groups": {"essential": true}}
                
                  rbacConfig:
                    policy.default: role:readonly
                    policy.csv: |
                      p, role:admin, applications, *, */*, allow
                      p, role:admin, clusters, *, *, allow
                      p, role:admin, repositories, *, *, allow
                      g, argocd-admins, role:admin
                
                controller:
                  replicas: 2
                  resources:
                    requests:
                      cpu: 250m
                      memory: 512Mi
                    limits:
                      cpu: 500m
                      memory: 1Gi
                
                repoServer:
                  replicas: 2
                  resources:
                    requests:
                      cpu: 250m
                      memory: 256Mi
                    limits:
                      cpu: 500m
                      memory: 512Mi
                
                redis:
                  resources:
                    requests:
                      cpu: 100m
                      memory: 128Mi
                    limits:
                      cpu: 200m
                      memory: 256Mi
            EOT,
        ]
      [32m+[0m[0m verify                     = false
      [32m+[0m[0m version                    = "5.51.4"
      [32m+[0m[0m wait                       = true
      [32m+[0m[0m wait_for_jobs              = false
    }

[1m  # helm_release.backstage[0m will be created
[0m  [32m+[0m[0m resource "helm_release" "backstage" {
      [32m+[0m[0m atomic                     = false
      [32m+[0m[0m chart                      = "backstage"
      [32m+[0m[0m cleanup_on_fail            = false
      [32m+[0m[0m create_namespace           = true
      [32m+[0m[0m dependency_update          = false
      [32m+[0m[0m disable_crd_hooks          = false
      [32m+[0m[0m disable_openapi_validation = false
      [32m+[0m[0m disable_webhooks           = false
      [32m+[0m[0m force_update               = false
      [32m+[0m[0m id                         = (known after apply)
      [32m+[0m[0m lint                       = false
      [32m+[0m[0m manifest                   = (known after apply)
      [32m+[0m[0m max_history                = 0
      [32m+[0m[0m metadata                   = (known after apply)
      [32m+[0m[0m name                       = "backstage"
      [32m+[0m[0m namespace                  = "backstage"
      [32m+[0m[0m pass_credentials           = false
      [32m+[0m[0m recreate_pods              = false
      [32m+[0m[0m render_subchart_notes      = true
      [32m+[0m[0m replace                    = false
      [32m+[0m[0m repository                 = "https://backstage.github.io/charts"
      [32m+[0m[0m reset_values               = false
      [32m+[0m[0m reuse_values               = false
      [32m+[0m[0m skip_crds                  = false
      [32m+[0m[0m status                     = "deployed"
      [32m+[0m[0m timeout                    = 300
      [32m+[0m[0m values                     = (known after apply)
      [32m+[0m[0m verify                     = false
      [32m+[0m[0m version                    = "1.9.0"
      [32m+[0m[0m wait                       = true
      [32m+[0m[0m wait_for_jobs              = false
    }

[1m  # helm_release.cert_manager[0m will be created
[0m  [32m+[0m[0m resource "helm_release" "cert_manager" {
      [32m+[0m[0m atomic                     = false
      [32m+[0m[0m chart                      = "cert-manager"
      [32m+[0m[0m cleanup_on_fail            = false
      [32m+[0m[0m create_namespace           = true
      [32m+[0m[0m dependency_update          = false
      [32m+[0m[0m disable_crd_hooks          = false
      [32m+[0m[0m disable_openapi_validation = false
      [32m+[0m[0m disable_webhooks           = false
      [32m+[0m[0m force_update               = false
      [32m+[0m[0m id                         = (known after apply)
      [32m+[0m[0m lint                       = false
      [32m+[0m[0m manifest                   = (known after apply)
      [32m+[0m[0m max_history                = 0
      [32m+[0m[0m metadata                   = (known after apply)
      [32m+[0m[0m name                       = "cert-manager"
      [32m+[0m[0m namespace                  = "cert-manager"
      [32m+[0m[0m pass_credentials           = false
      [32m+[0m[0m recreate_pods              = false
      [32m+[0m[0m render_subchart_notes      = true
      [32m+[0m[0m replace                    = false
      [32m+[0m[0m repository                 = "https://charts.jetstack.io"
      [32m+[0m[0m reset_values               = false
      [32m+[0m[0m reuse_values               = false
      [32m+[0m[0m skip_crds                  = false
      [32m+[0m[0m status                     = "deployed"
      [32m+[0m[0m timeout                    = 300
      [32m+[0m[0m verify                     = false
      [32m+[0m[0m version                    = "v1.13.3"
      [32m+[0m[0m wait                       = true
      [32m+[0m[0m wait_for_jobs              = false

      [32m+[0m[0m set {
          [32m+[0m[0m name  = "global.leaderElection.namespace"
          [32m+[0m[0m value = "cert-manager"
            [90m# (1 unchanged attribute hidden)[0m[0m
        }
      [32m+[0m[0m set {
          [32m+[0m[0m name  = "installCRDs"
          [32m+[0m[0m value = "true"
            [90m# (1 unchanged attribute hidden)[0m[0m
        }
    }

[1m  # helm_release.minio[0][0m will be created
[0m  [32m+[0m[0m resource "helm_release" "minio" {
      [32m+[0m[0m atomic                     = false
      [32m+[0m[0m chart                      = "minio"
      [32m+[0m[0m cleanup_on_fail            = false
      [32m+[0m[0m create_namespace           = true
      [32m+[0m[0m dependency_update          = false
      [32m+[0m[0m disable_crd_hooks          = false
      [32m+[0m[0m disable_openapi_validation = false
      [32m+[0m[0m disable_webhooks           = false
      [32m+[0m[0m force_update               = false
      [32m+[0m[0m id                         = (known after apply)
      [32m+[0m[0m lint                       = false
      [32m+[0m[0m manifest                   = (known after apply)
      [32m+[0m[0m max_history                = 0
      [32m+[0m[0m metadata                   = (known after apply)
      [32m+[0m[0m name                       = "minio"
      [32m+[0m[0m namespace                  = "kratix-platform-system"
      [32m+[0m[0m pass_credentials           = false
      [32m+[0m[0m recreate_pods              = false
      [32m+[0m[0m render_subchart_notes      = true
      [32m+[0m[0m replace                    = false
      [32m+[0m[0m repository                 = "https://charts.min.io"
      [32m+[0m[0m reset_values               = false
      [32m+[0m[0m reuse_values               = false
      [32m+[0m[0m skip_crds                  = false
      [32m+[0m[0m status                     = "deployed"
      [32m+[0m[0m timeout                    = 300
      [32m+[0m[0m values                     = (known after apply)
      [32m+[0m[0m verify                     = false
      [32m+[0m[0m version                    = "5.0.14"
      [32m+[0m[0m wait                       = true
      [32m+[0m[0m wait_for_jobs              = false
    }

[1m  # helm_release.postgresql_backstage[0m will be created
[0m  [32m+[0m[0m resource "helm_release" "postgresql_backstage" {
      [32m+[0m[0m atomic                     = false
      [32m+[0m[0m chart                      = "postgresql"
      [32m+[0m[0m cleanup_on_fail            = false
      [32m+[0m[0m create_namespace           = true
      [32m+[0m[0m dependency_update          = false
      [32m+[0m[0m disable_crd_hooks          = false
      [32m+[0m[0m disable_openapi_validation = false
      [32m+[0m[0m disable_webhooks           = false
      [32m+[0m[0m force_update               = false
      [32m+[0m[0m id                         = (known after apply)
      [32m+[0m[0m lint                       = false
      [32m+[0m[0m manifest                   = (known after apply)
      [32m+[0m[0m max_history                = 0
      [32m+[0m[0m metadata                   = (known after apply)
      [32m+[0m[0m name                       = "backstage-postgresql"
      [32m+[0m[0m namespace                  = "backstage"
      [32m+[0m[0m pass_credentials           = false
      [32m+[0m[0m recreate_pods              = false
      [32m+[0m[0m render_subchart_notes      = true
      [32m+[0m[0m replace                    = false
      [32m+[0m[0m repository                 = "https://charts.bitnami.com/bitnami"
      [32m+[0m[0m reset_values               = false
      [32m+[0m[0m reuse_values               = false
      [32m+[0m[0m skip_crds                  = false
      [32m+[0m[0m status                     = "deployed"
      [32m+[0m[0m timeout                    = 300
      [32m+[0m[0m values                     = (known after apply)
      [32m+[0m[0m verify                     = false
      [32m+[0m[0m version                    = "13.2.24"
      [32m+[0m[0m wait                       = true
      [32m+[0m[0m wait_for_jobs              = false
    }

[1m  # kubectl_manifest.argocd_app_of_apps[0m will be created
[0m  [32m+[0m[0m resource "kubectl_manifest" "argocd_app_of_apps" {
      [32m+[0m[0m api_version             = "argoproj.io/v1alpha1"
      [32m+[0m[0m apply_only              = false
      [32m+[0m[0m force_conflicts         = false
      [32m+[0m[0m force_new               = false
      [32m+[0m[0m id                      = (known after apply)
      [32m+[0m[0m kind                    = "Application"
      [32m+[0m[0m live_manifest_incluster = (sensitive value)
      [32m+[0m[0m live_uid                = (known after apply)
      [32m+[0m[0m name                    = "app-of-apps"
      [32m+[0m[0m namespace               = "argocd"
      [32m+[0m[0m server_side_apply       = false
      [32m+[0m[0m uid                     = (known after apply)
      [32m+[0m[0m validate_schema         = true
      [32m+[0m[0m wait_for_rollout        = true
      [32m+[0m[0m yaml_body               = (sensitive value)
      [32m+[0m[0m yaml_body_parsed        = <<-EOT
            apiVersion: argoproj.io/v1alpha1
            kind: Application
            metadata:
              finalizers:
              - resources-finalizer.argocd.argoproj.io
              name: app-of-apps
              namespace: argocd
            spec:
              destination:
                namespace: argocd
                server: https://kubernetes.default.svc
              project: default
              source:
                path: applications
                repoURL: https://github.com/infraforge/platform-gitops
                targetRevision: main
              syncPolicy:
                automated:
                  allowEmpty: false
                  prune: true
                  selfHeal: true
                retry:
                  backoff:
                    duration: 5s
                    factor: 2
                    maxDuration: 3m
                  limit: 5
                syncOptions:
                - CreateNamespace=true
        EOT
      [32m+[0m[0m yaml_incluster          = (sensitive value)
    }

[1m  # kubectl_manifest.kratix_platform[0m will be created
[0m  [32m+[0m[0m resource "kubectl_manifest" "kratix_platform" {
      [32m+[0m[0m api_version             = "v1"
      [32m+[0m[0m apply_only              = false
      [32m+[0m[0m force_conflicts         = false
      [32m+[0m[0m force_new               = false
      [32m+[0m[0m id                      = (known after apply)
      [32m+[0m[0m kind                    = "Namespace"
      [32m+[0m[0m live_manifest_incluster = (sensitive value)
      [32m+[0m[0m live_uid                = (known after apply)
      [32m+[0m[0m name                    = "kratix-platform-system"
      [32m+[0m[0m namespace               = (known after apply)
      [32m+[0m[0m server_side_apply       = false
      [32m+[0m[0m uid                     = (known after apply)
      [32m+[0m[0m validate_schema         = true
      [32m+[0m[0m wait_for_rollout        = true
      [32m+[0m[0m yaml_body               = (sensitive value)
      [32m+[0m[0m yaml_body_parsed        = <<-EOT
            apiVersion: v1
            kind: Namespace
            metadata:
              name: kratix-platform-system
        EOT
      [32m+[0m[0m yaml_incluster          = (sensitive value)
    }

[1m  # kubectl_manifest.kratix_postgresql_promise[0m will be created
[0m  [32m+[0m[0m resource "kubectl_manifest" "kratix_postgresql_promise" {
      [32m+[0m[0m api_version             = "platform.kratix.io/v1alpha1"
      [32m+[0m[0m apply_only              = false
      [32m+[0m[0m force_conflicts         = false
      [32m+[0m[0m force_new               = false
      [32m+[0m[0m id                      = (known after apply)
      [32m+[0m[0m kind                    = "Promise"
      [32m+[0m[0m live_manifest_incluster = (sensitive value)
      [32m+[0m[0m live_uid                = (known after apply)
      [32m+[0m[0m name                    = "postgresql"
      [32m+[0m[0m namespace               = "kratix-platform-system"
      [32m+[0m[0m server_side_apply       = false
      [32m+[0m[0m uid                     = (known after apply)
      [32m+[0m[0m validate_schema         = true
      [32m+[0m[0m wait_for_rollout        = true
      [32m+[0m[0m yaml_body               = (sensitive value)
      [32m+[0m[0m yaml_body_parsed        = <<-EOT
            apiVersion: platform.kratix.io/v1alpha1
            kind: Promise
            metadata:
              name: postgresql
              namespace: kratix-platform-system
            spec:
              api:
                apiVersion: apiextensions.k8s.io/v1
                kind: CustomResourceDefinition
                metadata:
                  name: postgresqls.marketplace.kratix.io
                spec:
                  group: marketplace.kratix.io
                  names:
                    kind: PostgreSQL
                    plural: postgresqls
                    singular: postgresql
                  scope: Namespaced
                  versions:
                  - name: v1alpha1
                    schema:
                      openAPIV3Schema:
                        properties:
                          spec:
                            properties:
                              dbName:
                                type: string
                              size:
                                enum:
                                - small
                                - medium
                                - large
                                type: string
                              version:
                                default: "14"
                                type: string
                            type: object
                        type: object
                    served: true
                    storage: true
              destinationSelectors:
              - matchLabels:
                  environment: dev
              workflows:
                resource:
                  configure:
                  - apiVersion: platform.kratix.io/v1alpha1
                    kind: Pipeline
                    metadata:
                      name: postgresql-configure
                    spec:
                      containers:
                      - command:
                        - sh
                        - -c
                        - |
                          cat <<EOF | kubectl apply -f -
                          apiVersion: apps/v1
                          kind: StatefulSet
                          metadata:
                            name: postgres-$(RESOURCE_NAME)
                            namespace: $(RESOURCE_NAMESPACE)
                          spec:
                            replicas: 1
                            selector:
                              matchLabels:
                                app: postgres-$(RESOURCE_NAME)
                            template:
                              metadata:
                                labels:
                                  app: postgres-$(RESOURCE_NAME)
                              spec:
                                containers:
                                - name: postgres
                                  image: postgres:$(RESOURCE_SPEC_VERSION)
                                  env:
                                  - name: POSTGRES_DB
                                    value: $(RESOURCE_SPEC_DBNAME)
                                  - name: POSTGRES_PASSWORD
                                    value: postgres123
                                  volumeMounts:
                                  - name: data
                                    mountPath: /var/lib/postgresql/data
                            volumeClaimTemplates:
                            - metadata:
                                name: data
                              spec:
                                accessModes: ["ReadWriteOnce"]
                                storageClassName: gp3
                                resources:
                                  requests:
                                    storage: 10Gi
                          EOF
                        image: alpine/k8s:1.28.4
                        name: create-resources
        EOT
      [32m+[0m[0m yaml_incluster          = (sensitive value)
    }

[1m  # kubectl_manifest.kratix_redis_promise[0m will be created
[0m  [32m+[0m[0m resource "kubectl_manifest" "kratix_redis_promise" {
      [32m+[0m[0m api_version             = "platform.kratix.io/v1alpha1"
      [32m+[0m[0m apply_only              = false
      [32m+[0m[0m force_conflicts         = false
      [32m+[0m[0m force_new               = false
      [32m+[0m[0m id                      = (known after apply)
      [32m+[0m[0m kind                    = "Promise"
      [32m+[0m[0m live_manifest_incluster = (sensitive value)
      [32m+[0m[0m live_uid                = (known after apply)
      [32m+[0m[0m name                    = "redis"
      [32m+[0m[0m namespace               = "kratix-platform-system"
      [32m+[0m[0m server_side_apply       = false
      [32m+[0m[0m uid                     = (known after apply)
      [32m+[0m[0m validate_schema         = true
      [32m+[0m[0m wait_for_rollout        = true
      [32m+[0m[0m yaml_body               = (sensitive value)
      [32m+[0m[0m yaml_body_parsed        = <<-EOT
            apiVersion: platform.kratix.io/v1alpha1
            kind: Promise
            metadata:
              name: redis
              namespace: kratix-platform-system
            spec:
              api:
                apiVersion: apiextensions.k8s.io/v1
                kind: CustomResourceDefinition
                metadata:
                  name: redis.marketplace.kratix.io
                spec:
                  group: marketplace.kratix.io
                  names:
                    kind: Redis
                    plural: redis
                    singular: redis
                  scope: Namespaced
                  versions:
                  - name: v1alpha1
                    schema:
                      openAPIV3Schema:
                        properties:
                          spec:
                            properties:
                              persistence:
                                default: true
                                type: boolean
                              size:
                                enum:
                                - small
                                - medium
                                - large
                                type: string
                            type: object
                        type: object
                    served: true
                    storage: true
              workflows:
                resource:
                  configure:
                  - apiVersion: platform.kratix.io/v1alpha1
                    kind: Pipeline
                    metadata:
                      name: redis-configure
                    spec:
                      containers:
                      - command:
                        - sh
                        - -c
                        - |
                          cat <<EOF | kubectl apply -f -
                          apiVersion: apps/v1
                          kind: Deployment
                          metadata:
                            name: redis-$(RESOURCE_NAME)
                            namespace: $(RESOURCE_NAMESPACE)
                          spec:
                            replicas: 1
                            selector:
                              matchLabels:
                                app: redis-$(RESOURCE_NAME)
                            template:
                              metadata:
                                labels:
                                  app: redis-$(RESOURCE_NAME)
                              spec:
                                containers:
                                - name: redis
                                  image: redis:7-alpine
                                  ports:
                                  - containerPort: 6379
                          ---
                          apiVersion: v1
                          kind: Service
                          metadata:
                            name: redis-$(RESOURCE_NAME)
                            namespace: $(RESOURCE_NAMESPACE)
                          spec:
                            selector:
                              app: redis-$(RESOURCE_NAME)
                            ports:
                            - port: 6379
                              targetPort: 6379
                          EOF
                        image: alpine/k8s:1.28.4
                        name: create-resources
        EOT
      [32m+[0m[0m yaml_incluster          = (sensitive value)
    }

[1m  # kubernetes_cluster_role.backstage[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_cluster_role" "backstage" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m name             = "backstage-reader"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m rule {
          [32m+[0m[0m api_groups = [
              [32m+[0m[0m [90mnull[0m[0m,
              [32m+[0m[0m "apps",
              [32m+[0m[0m "batch",
              [32m+[0m[0m "networking.k8s.io",
            ]
          [32m+[0m[0m resources  = [
              [32m+[0m[0m "pods",
              [32m+[0m[0m "services",
              [32m+[0m[0m "deployments",
              [32m+[0m[0m "replicasets",
              [32m+[0m[0m "ingresses",
              [32m+[0m[0m "jobs",
              [32m+[0m[0m "cronjobs",
            ]
          [32m+[0m[0m verbs      = [
              [32m+[0m[0m "get",
              [32m+[0m[0m "list",
              [32m+[0m[0m "watch",
            ]
        }
      [32m+[0m[0m rule {
          [32m+[0m[0m api_groups = [
              [32m+[0m[0m "platform.kratix.io",
              [32m+[0m[0m "marketplace.kratix.io",
            ]
          [32m+[0m[0m resources  = [
              [32m+[0m[0m "*",
            ]
          [32m+[0m[0m verbs      = [
              [32m+[0m[0m "get",
              [32m+[0m[0m "list",
              [32m+[0m[0m "watch",
              [32m+[0m[0m "create",
              [32m+[0m[0m "update",
              [32m+[0m[0m "patch",
            ]
        }
    }

[1m  # kubernetes_cluster_role_binding.backstage[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_cluster_role_binding" "backstage" {
      [32m+[0m[0m id = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m name             = "backstage-reader"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m role_ref {
          [32m+[0m[0m api_group = "rbac.authorization.k8s.io"
          [32m+[0m[0m kind      = "ClusterRole"
          [32m+[0m[0m name      = "backstage-reader"
        }

      [32m+[0m[0m subject {
          [32m+[0m[0m api_group = (known after apply)
          [32m+[0m[0m kind      = "ServiceAccount"
          [32m+[0m[0m name      = "backstage"
          [32m+[0m[0m namespace = "backstage"
        }
    }

[1m  # kubernetes_secret.argocd_admin_password[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_secret" "argocd_admin_password" {
      [32m+[0m[0m data                           = (sensitive value)
      [32m+[0m[0m id                             = (known after apply)
      [32m+[0m[0m type                           = "Opaque"
      [32m+[0m[0m wait_for_service_account_token = true

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m name             = "argocd-initial-admin-secret"
          [32m+[0m[0m namespace        = "argocd"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }
    }

[1m  # kubernetes_service_account.backstage[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_service_account" "backstage" {
      [32m+[0m[0m automount_service_account_token = true
      [32m+[0m[0m default_secret_name             = (known after apply)
      [32m+[0m[0m id                              = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m name             = "backstage"
          [32m+[0m[0m namespace        = "backstage"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }
    }

[1m  # null_resource.configure_ebs_csi[0][0m will be created
[0m  [32m+[0m[0m resource "null_resource" "configure_ebs_csi" {
      [32m+[0m[0m id = (known after apply)
    }

[1m  # random_password.argocd_admin[0m will be created
[0m  [32m+[0m[0m resource "random_password" "argocd_admin" {
      [32m+[0m[0m bcrypt_hash = (sensitive value)
      [32m+[0m[0m id          = (known after apply)
      [32m+[0m[0m length      = 16
      [32m+[0m[0m lower       = true
      [32m+[0m[0m min_lower   = 0
      [32m+[0m[0m min_numeric = 0
      [32m+[0m[0m min_special = 0
      [32m+[0m[0m min_upper   = 0
      [32m+[0m[0m number      = true
      [32m+[0m[0m numeric     = true
      [32m+[0m[0m result      = (sensitive value)
      [32m+[0m[0m special     = true
      [32m+[0m[0m upper       = true
    }

[1m  # random_password.backstage_db_password[0m will be created
[0m  [32m+[0m[0m resource "random_password" "backstage_db_password" {
      [32m+[0m[0m bcrypt_hash = (sensitive value)
      [32m+[0m[0m id          = (known after apply)
      [32m+[0m[0m length      = 16
      [32m+[0m[0m lower       = true
      [32m+[0m[0m min_lower   = 0
      [32m+[0m[0m min_numeric = 0
      [32m+[0m[0m min_special = 0
      [32m+[0m[0m min_upper   = 0
      [32m+[0m[0m number      = true
      [32m+[0m[0m numeric     = true
      [32m+[0m[0m result      = (sensitive value)
      [32m+[0m[0m special     = false
      [32m+[0m[0m upper       = true
    }

[1m  # random_password.minio_password[0][0m will be created
[0m  [32m+[0m[0m resource "random_password" "minio_password" {
      [32m+[0m[0m bcrypt_hash = (sensitive value)
      [32m+[0m[0m id          = (known after apply)
      [32m+[0m[0m length      = 16
      [32m+[0m[0m lower       = true
      [32m+[0m[0m min_lower   = 0
      [32m+[0m[0m min_numeric = 0
      [32m+[0m[0m min_special = 0
      [32m+[0m[0m min_upper   = 0
      [32m+[0m[0m number      = true
      [32m+[0m[0m numeric     = true
      [32m+[0m[0m result      = (sensitive value)
      [32m+[0m[0m special     = false
      [32m+[0m[0m upper       = true
    }

[1m  # module.eks.kubernetes_config_map_v1_data.aws_auth[0][0m will be created
[0m  [32m+[0m[0m resource "kubernetes_config_map_v1_data" "aws_auth" {
      [32m+[0m[0m data          = {
          [32m+[0m[0m "mapAccounts" = jsonencode([])
          [32m+[0m[0m "mapRoles"    = <<-EOT
                - "groups":
                  - "system:bootstrappers"
                  - "system:nodes"
                  "rolearn": "arn:aws:iam::715841344657:role/infraforge-dev-general-eks-node-group-20251217170752624000000007"
                  "username": "system:node:{{EC2PrivateDNSName}}"
                - "groups":
                  - "system:bootstrappers"
                  - "system:nodes"
                  "rolearn": "arn:aws:iam::715841344657:role/infraforge-dev-system-eks-node-group-20251217170752624100000008"
                  "username": "system:node:{{EC2PrivateDNSName}}"
                - "groups":
                  - "system:bootstrappers"
                  - "system:nodes"
                  - "system:node-proxier"
                  "rolearn": "arn:aws:iam::715841344657:role/system-20251217170752623600000006"
                  "username": "system:node:{{SessionName}}"
                - "groups":
                  - "system:masters"
                  "rolearn": "arn:aws:iam::715841344657:role/Admin"
                  "username": "admin"
            EOT
          [32m+[0m[0m "mapUsers"    = jsonencode([])
        }
      [32m+[0m[0m field_manager = "Terraform"
      [32m+[0m[0m force         = true
      [32m+[0m[0m id            = (known after apply)

      [32m+[0m[0m metadata {
          [32m+[0m[0m name      = "aws-auth"
          [32m+[0m[0m namespace = "kube-system"
        }
    }

[1m  # module.eks.time_sleep.this[0] (deposed object fca075fe)[0m will be [1m[31mdestroyed[0m
  # (left over from a partially-failed replacement of this instance)
[0m  [31m-[0m[0m resource "time_sleep" "this" {
      [31m-[0m[0m create_duration = "30s" [90m-> null[0m[0m
      [31m-[0m[0m id              = "2025-12-17T17:18:25Z" [90m-> null[0m[0m
      [31m-[0m[0m triggers        = {
          [31m-[0m[0m "cluster_certificate_authority_data" = "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJTytscUVweit1dE13RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TlRFeU1UY3hOekE1TURsYUZ3MHpOVEV5TVRVeE56RTBNRGxhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUN5ckFab3JYSHA0UzBIMS9LWENKUmNnR2M2bWhnV3dST1NLRjBYTDNwU09YT20vY01uTEZIQUV2YUsKOWpoL3JFcU93UjRPTXljaUxnWFNodlJuZHkrblA4M2hFdTlDQjY3UzlTdXM1SW84ZVNuOXQ4ZGVxVUlKMU84Nwp1UUIySkp0SHlFdjIvK3gyY1ZkbXZKUXpyRExiREdEUXRkbUZVWWJ6U0RScVRrY3NPRWQ5RHUvNGJTL0dROWlQClBjOGU1STBhdlRwanU1ODU2L3hoWHFMNytaeTRGc3VDRGhRZU1ZU0lFSjVFb3J1clR6eGV6Rm1GU09iQThSQ0EKU3Fxank2SWZ1TEFDdUdlTXg2UDFTZHloWDJkRjRaQTIxTjJ0ZjEreTJFMnRrQjg3VEJlbHBtVVhiYjZVRjQweApNTUl3akJWU3FsK211MWZ2aUkzbllRTysyblVuQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJSUkpiVytrNzVBdXh0RE1rZk9QYTR5dExVVmRqQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQXllek1QdzdQQgpXU2FIcHpKc2xaNCtJZjR0UVJpNlhFVjFPbHZDUG9DdEVpQ1ZqdmE4NXFTcjhCU00rWGxSVEs2VHMxcjM1eFczCm0zQWtXUXdERW9XNk5qZmM3ejhCUWdVdm90QnJLb1l6VUJuaWJkbFpHUlJncFJ1U2NtbjVxS2JrdXN3dCtPTEQKVU5YVlpya3JoS1d2OFVYZ2M4bUthcmNkWVhCYSt3RHVhRW03MWN2NTh5NEYzamxYUnVPbjJhZEl6Z0VUSXVaTgowMGMyMSthZEV5WlI1MitZNHV1QkkxaWtuTlBFZDcyUW5KQjRTbk0zYTlqZWV1TW1hTjZvd2pmNkxBMk1sUUxiCkx5ZlJBV3lFRmxkTEVhSGVwODZId3JxcTBxcHFubmF2bGloRkR4dGFKSUluY2RXeHBsVVRpQWZpeW8xa09ESDMKc3dXRFpIdERHcVJpCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K"
          [31m-[0m[0m "cluster_endpoint"                   = "https://ADDA703F782CABA7FEBFF9FC355069D1.sk1.eu-west-1.eks.amazonaws.com"
          [31m-[0m[0m "cluster_name"                       = "infraforge-dev"
          [31m-[0m[0m "cluster_version"                    = "1.28"
        } [90m-> null[0m[0m
    }

[1mPlan:[0m 18 to add, 0 to change, 1 to destroy.
[0m
Changes to Outputs:
  [32m+[0m[0m argocd_admin_password                 = (sensitive value)
  [32m+[0m[0m argocd_url                            = "https://argocd.infraforge.io"
  [32m+[0m[0m backstage_db_password                 = (sensitive value)
  [32m+[0m[0m backstage_url                         = "https://backstage.infraforge.io"
  [32m+[0m[0m kratix_minio_console_url              = "Check LoadBalancer IP for MinIO console"
  [32m+[0m[0m kratix_minio_password                 = (sensitive value)
[0m[1mrandom_password.minio_password[0]: Creating...[0m[0m
[0m[1mrandom_password.argocd_admin: Creating...[0m[0m
[0m[1mrandom_password.backstage_db_password: Creating...[0m[0m
[0m[1mrandom_password.argocd_admin: Creation complete after 0s [id=none][0m
[0m[1mrandom_password.minio_password[0]: Creation complete after 0s [id=none][0m
[0m[1mrandom_password.backstage_db_password: Creation complete after 0s [id=none][0m
[0m[1mkubernetes_service_account.backstage: Creating...[0m[0m
[0m[1mmodule.eks.kubernetes_config_map_v1_data.aws_auth[0]: Creating...[0m[0m
[0m[1mkubernetes_cluster_role.backstage: Creating...[0m[0m
[0m[1mkubernetes_cluster_role.backstage: Creation complete after 1s [id=backstage-reader][0m
[0m[1mmodule.eks.kubernetes_config_map_v1_data.aws_auth[0]: Creation complete after 1s [id=kube-system/aws-auth][0m
[0m[1mnull_resource.configure_ebs_csi[0]: Creating...[0m[0m
[0m[1mnull_resource.configure_ebs_csi[0]: Provisioning with 'local-exec'...[0m[0m
[0m[1mnull_resource.configure_ebs_csi[0] (local-exec):[0m [0mExecuting: ["/bin/sh" "-c" "aws eks update-kubeconfig --name infraforge-dev --region eu-west-1\n\n# Wait for EBS CSI driver to be ready\nkubectl wait --for=condition=Ready pods -l app=ebs-csi-controller -n kube-system --timeout=300s || true\n\n# Create default storage class if it doesn't exist\nkubectl apply -f - <<EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: gp3\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  fsType: ext4\n  encrypted: \"true\"\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nreclaimPolicy: Delete\nEOF\n"]
[0m[1mnull_resource.configure_ebs_csi[0] (local-exec):[0m [0mUpdated context arn:aws:eks:eu-west-1:715841344657:cluster/infraforge-dev in /Users/gaskin/.kube/config
[0m[1mnull_resource.configure_ebs_csi[0] (local-exec):[0m [0mpod/ebs-csi-controller-5548dc865b-j9bgf condition met
[0m[1mnull_resource.configure_ebs_csi[0] (local-exec):[0m [0mpod/ebs-csi-controller-5548dc865b-q22lm condition met
[0m[1mnull_resource.configure_ebs_csi[0] (local-exec):[0m [0mstorageclass.storage.k8s.io/gp3 configured
[0m[1mnull_resource.configure_ebs_csi[0]: Creation complete after 8s [id=5410801346884088876][0m
[0m[1mhelm_release.postgresql_backstage: Creating...[0m[0m
[0m[1mhelm_release.argocd: Creating...[0m[0m
[0m[1mhelm_release.cert_manager: Creating...[0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [10s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [10s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [10s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [20s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [20s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [30s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [30s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [30s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [40s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [40s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [40s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [50s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [50s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [50s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [1m0s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [1m0s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [1m0s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [1m10s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [1m10s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [1m10s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [1m20s elapsed][0m[0m
[0m[1mhelm_release.argocd: Still creating... [1m20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [1m20s elapsed][0m[0m
[0m[1mhelm_release.argocd: Creation complete after 1m26s [id=argocd][0m
[0m[1mkubectl_manifest.argocd_app_of_apps: Creating...[0m[0m
[0m[1mkubernetes_secret.argocd_admin_password: Creating...[0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [1m30s elapsed][0m[0m
[0m[1mkubectl_manifest.argocd_app_of_apps: Creation complete after 3s [id=/apis/argoproj.io/v1alpha1/namespaces/argocd/applications/app-of-apps][0m
[0m[1mhelm_release.cert_manager: Still creating... [1m30s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [1m40s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [1m40s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [1m50s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [1m50s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [2m0s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [2m0s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [2m10s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [2m10s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [2m20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [2m20s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [2m30s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [2m30s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [2m40s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [2m40s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [2m50s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [2m50s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [3m0s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [3m0s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [3m10s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [3m10s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [3m20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [3m20s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [3m30s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [3m30s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [3m40s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [3m40s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [3m50s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [3m50s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [4m0s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [4m0s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [4m10s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [4m10s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [4m20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [4m20s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [4m30s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [4m30s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [4m40s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [4m40s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [4m50s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [4m50s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [5m0s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [5m0s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [5m10s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [5m10s elapsed][0m[0m
[0m[1mhelm_release.postgresql_backstage: Still creating... [5m20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [5m20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [5m30s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [5m40s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [5m50s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [6m0s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [6m10s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [6m20s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [6m30s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [6m40s elapsed][0m[0m
[0m[1mhelm_release.cert_manager: Still creating... [6m50s elapsed][0m[0m
[33m╷[0m[0m
[33m│[0m [0m[1m[33mWarning: [0m[0m[1mValue for undeclared variable[0m
[33m│[0m [0m
[33m│[0m [0m[0mThe root module does not declare a variable named "enable_monitoring" but a
[33m│[0m [0mvalue was found in file "terraform.tfvars". If you meant to use this value,
[33m│[0m [0madd a "variable" block to the configuration.
[33m│[0m [0m
[33m│[0m [0mTo silence these warnings, use TF_VAR_... environment variables to provide
[33m│[0m [0mcertain "global" settings to all configurations in your organization. To
[33m│[0m [0mreduce the verbosity of these warnings, use the -compact-warnings option.
[33m╵[0m[0m
[33m╷[0m[0m
[33m│[0m [0m[1m[33mWarning: [0m[0m[1mValue for undeclared variable[0m
[33m│[0m [0m
[33m│[0m [0m[0mThe root module does not declare a variable named "monitoring_config" but a
[33m│[0m [0mvalue was found in file "terraform.tfvars". If you meant to use this value,
[33m│[0m [0madd a "variable" block to the configuration.
[33m│[0m [0m
[33m│[0m [0mTo silence these warnings, use TF_VAR_... environment variables to provide
[33m│[0m [0mcertain "global" settings to all configurations in your organization. To
[33m│[0m [0mreduce the verbosity of these warnings, use the -compact-warnings option.
[33m╵[0m[0m
[33m╷[0m[0m
[33m│[0m [0m[1m[33mWarning: [0m[0m[1mHelm release "" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again.[0m
[33m│[0m [0m
[33m│[0m [0m[0m  with helm_release.postgresql_backstage,
[33m│[0m [0m  on backstage.tf line 4, in resource "helm_release" "postgresql_backstage":
[33m│[0m [0m   4: resource "helm_release" "postgresql_backstage" [4m{[0m[0m
[33m│[0m [0m
[33m│[0m [0m(and one more similar warning elsewhere)
[33m╵[0m[0m
[33m╷[0m[0m
[33m│[0m [0m[1m[33mWarning: [0m[0m[1mDeprecated attribute[0m
[33m│[0m [0m
[33m│[0m [0m[0m  on backstage.tf line 228, in data "kubernetes_secret" "backstage_sa_token":
[33m│[0m [0m 228:     name      = kubernetes_service_account.backstage[4m.default_secret_name[0m[0m
[33m│[0m [0m
[33m│[0m [0mThe attribute "default_secret_name" is deprecated. Refer to the provider
[33m│[0m [0mdocumentation for details.
[33m│[0m [0m
[33m│[0m [0m(and one more similar warning elsewhere)
[33m╵[0m[0m
[33m╷[0m[0m
[33m│[0m [0m[1m[33mWarning: [0m[0m[1mArgument is deprecated[0m
[33m│[0m [0m
[33m│[0m [0m[0m  with module.eks.aws_iam_role.this[0],
[33m│[0m [0m  on .terraform/modules/eks/main.tf line 293, in resource "aws_iam_role" "this":
[33m│[0m [0m 293: resource "aws_iam_role" "this" [4m{[0m[0m
[33m│[0m [0m
[33m│[0m [0minline_policy is deprecated. Use the aws_iam_role_policy resource instead.
[33m│[0m [0mIf Terraform should exclusively manage all inline policy associations (the
[33m│[0m [0mcurrent behavior of this argument), use the aws_iam_role_policies_exclusive
[33m│[0m [0mresource as well.
[33m│[0m [0m
[33m│[0m [0m(and 4 more similar warnings elsewhere)
[33m╵[0m[0m
[31m╷[0m[0m
[31m│[0m [0m[1m[31mError: [0m[0m[1msecrets "argocd-initial-admin-secret" already exists[0m
[31m│[0m [0m
[31m│[0m [0m[0m  with kubernetes_secret.argocd_admin_password,
[31m│[0m [0m  on argocd.tf line 84, in resource "kubernetes_secret" "argocd_admin_password":
[31m│[0m [0m  84: resource "kubernetes_secret" "argocd_admin_password" [4m{[0m[0m
[31m│[0m [0m
[31m╵[0m[0m
[31m╷[0m[0m
[31m│[0m [0m[1m[31mError: [0m[0m[1mcontext deadline exceeded[0m
[31m│[0m [0m
[31m│[0m [0m[0m  with helm_release.postgresql_backstage,
[31m│[0m [0m  on backstage.tf line 4, in resource "helm_release" "postgresql_backstage":
[31m│[0m [0m   4: resource "helm_release" "postgresql_backstage" [4m{[0m[0m
[31m│[0m [0m
[31m╵[0m[0m
[31m╷[0m[0m
[31m│[0m [0m[1m[31mError: [0m[0m[1mnamespaces "backstage" not found[0m
[31m│[0m [0m
[31m│[0m [0m[0m  with kubernetes_service_account.backstage,
[31m│[0m [0m  on backstage.tf line 182, in resource "kubernetes_service_account" "backstage":
[31m│[0m [0m 182: resource "kubernetes_service_account" "backstage" [4m{[0m[0m
[31m│[0m [0m
[31m╵[0m[0m
[31m╷[0m[0m
[31m│[0m [0m[1m[31mError: [0m[0m[1mfailed post-install: 1 error occurred:
[31m│[0m [0m	* timed out waiting for the condition
[31m│[0m [0m
[31m│[0m [0m[0m
[31m│[0m [0m
[31m│[0m [0m[0m  with helm_release.cert_manager,
[31m│[0m [0m  on kratix.tf line 2, in resource "helm_release" "cert_manager":
[31m│[0m [0m   2: resource "helm_release" "cert_manager" [4m{[0m[0m
[31m│[0m [0m
[31m╵[0m[0m
